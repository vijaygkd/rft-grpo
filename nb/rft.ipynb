{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Model\n",
    "- Gemma 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Model name: google/gemma-3-1b-it\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"google/gemma-3-1b-it\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model name: {model_name}\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   108 | \n",
      "\n",
      "       | 0.0000 | 100.00%\n",
      "| 11355 | Why      | 0.0000 | 100.00%\n",
      "|  1602 |  did     | 0.0000 | 100.00%\n",
      "|   506 |  the     | 0.0000 | 100.00%\n",
      "| 30979 |  bicycle | -1.0309 | 35.67%\n",
      "|  3798 |  fall    | 0.0000 | 100.00%\n",
      "|  1024 |  over    | 0.0000 | 100.00%\n",
      "| 236881 | ?        | 0.0000 | 100.00%\n",
      "|   108 | \n",
      "\n",
      "       | -0.0537 | 94.77%\n",
      "| 17574 | Because  | 0.0000 | 100.00%\n",
      "Response: \n",
      "\n",
      "Why did the bicycle fall over?\n",
      "\n",
      "Because\n",
      "Probability of the response: 33.80%\n",
      "|   108 | \n",
      "\n",
      "       | 0.0000 | 100.00%\n",
      "| 11355 | Why      | 0.0000 | 100.00%\n",
      "|  1602 |  did     | 0.0000 | 100.00%\n",
      "|   506 |  the     | 0.0000 | 100.00%\n",
      "| 55134 |  scare   | -0.4411 | 64.33%\n",
      "| 47129 | crow     | 0.0000 | 100.00%\n",
      "|  3345 |  win     | 0.0000 | 100.00%\n",
      "|   614 |  an      | 0.0000 | 100.00%\n",
      "|  8054 |  award   | 0.0000 | 100.00%\n",
      "| 236881 | ?        | 0.0000 | 100.00%\n",
      "Response: \n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "Probability of the response: 64.33%\n",
      "\n",
      "\n",
      "Why did the scarecrow win an award?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_response(prompt, num_return_sequences=2):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=10,\n",
    "        temperature=0.5,      # sample generations\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "    )\n",
    "    transition_scores = model.compute_transition_scores(\n",
    "        outputs.sequences, outputs.scores, normalize_logits=True\n",
    "    )\n",
    "\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    for i in range(generated_tokens.shape[0]):\n",
    "        sum_logits = 0\n",
    "        for tok, score in zip(generated_tokens[i], transition_scores[i]):\n",
    "            # | token | token string | logits | probability\n",
    "            score = score.cpu().numpy()\n",
    "            sum_logits += score\n",
    "            print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score:.4f} | {np.exp(score):.2%}\")\n",
    "\n",
    "        response = tokenizer.decode(generated_tokens[i], skip_special_tokens=True)\n",
    "        print(f\"Response: {response}\")\n",
    "        print(f\"Probability of the response: {np.exp(sum_logits):.2%}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(generate_response(\"Tell me a joke.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.configuration_utils import GenerationConfig\n",
    "\n",
    "gc = GenerationConfig(\n",
    "    do_sample=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1: Tell me a joke.\n",
      "\n",
      "Why did the bicycle fall over?\n",
      "...\n",
      "Probability: 0.79%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_input = \"Tell me a joke.\"\n",
    "input_pt = tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "input_len = input_pt.input_ids.shape[1]\n",
    "\n",
    "sample_seqs =model.generate(\n",
    "    **input_pt,\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.5,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    sample_seqs.sequences, sample_seqs.scores, normalize_logits=True\n",
    ")\n",
    "\n",
    "for i in range(sample_seqs.sequences.shape[0]):\n",
    "    print(f\"Sequence {i+1}: {tokenizer.decode(sample_seqs.sequences[i], skip_special_tokens=True)}\")\n",
    "    print(f\"Probability: {torch.exp(transition_scores[i].sum()):.2%}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "\n",
      "\"\t| token 108 prob: 100.00% | log prob: 0.0\n",
      "\"Why\"\t| token 11355 prob: 100.00% | log prob: 0.0\n",
      "\" did\"\t| token 1602 prob: 100.00% | log prob: 0.0\n",
      "\" the\"\t| token 506 prob: 100.00% | log prob: 0.0\n",
      "\" bicycle\"\t| token 30979 prob: 35.67% | log prob: -1.031\n",
      "\" fall\"\t| token 3798 prob: 100.00% | log prob: 0.0\n",
      "\" over\"\t| token 1024 prob: 100.00% | log prob: 0.0\n",
      "\"?\"\t| token 236881 prob: 100.00% | log prob: 0.0\n",
      "\"\n",
      "\"\t| token 107 prob: 5.23% | log prob: -2.951\n",
      "\"...\"\t| token 1390 prob: 42.42% | log prob: -0.8576\n",
      "Sequence 1 log prob: -4.84\n",
      "Sequence 1 prob: 0.79%\n",
      "Sequence 1 string: \n",
      "\n",
      "Why did the bicycle fall over?\n",
      "...\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(sample_seqs.sequences.shape[0]):  # no. of sequences\n",
    "    sum_log_prob = 0\n",
    "    seq_str = \"\"\n",
    "    for j in range(sample_seqs.sequences[i].shape[0]-input_len):  # output time step\n",
    "        # max_idx = torch.argmax(sample_seqs.scores[j][i])  # list of score per time stamp\n",
    "        max_idx = sample_seqs.sequences[i][j+input_len] # sampled token\n",
    "        log_probs = torch.log_softmax(sample_seqs.scores[j][i], dim=-1)\n",
    "        sum_log_prob += log_probs[max_idx]\n",
    "        gen_token = tokenizer.decode(max_idx)\n",
    "        seq_str += gen_token\n",
    "        print(f\"\\\"{gen_token}\\\"\\t| token {max_idx} prob: {torch.exp(log_probs[max_idx]):.2%} | log prob: {log_probs[max_idx]:.4}\")\n",
    "    print(f\"Sequence {i+1} log prob: {sum_log_prob:.4}\")\n",
    "    print(f\"Sequence {i+1} prob: {torch.exp(sum_log_prob):.2%}\")\n",
    "    print(f\"Sequence {i+1} string: {seq_str}\")\n",
    "    print(\"-\"*100)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "### 3. Reward function\n",
    "\n",
    "- Reward function for wordle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_wordle(y_true: str, y_pred: str) -> float:\n",
    "    \"\"\"\n",
    "    Reward function to calculate reward for wordle guess.\n",
    "    Heuristic:\n",
    "    if y_pred is invalid (!=5 or non-alpha) : 0\n",
    "    for each position correct : 0.1 x n\n",
    "    for each alpha correct in wrong position: 0.05 x n\n",
    "    if extact match : 1\n",
    "    \"\"\"\n",
    "    y_true = y_true.strip().upper()\n",
    "    y_pred = y_pred.strip().upper()\n",
    "    # Check validity\n",
    "    if len(y_pred) != 5 or not y_pred.isalpha():\n",
    "        return 0.0\n",
    "    if y_pred == y_true:\n",
    "        return 1.0\n",
    "    reward = 0.0\n",
    "    # Count correct positions\n",
    "    for i in range(5):\n",
    "        if y_pred[i] == y_true[i]:\n",
    "            reward += 0.1\n",
    "    # Count correct letters in wrong positions\n",
    "    # To avoid double-counting, mark matched positions\n",
    "    true_counts = {}\n",
    "    pred_counts = {}\n",
    "    for i in range(5):\n",
    "        if y_pred[i] != y_true[i]:\n",
    "            true_counts[y_true[i]] = true_counts.get(y_true[i], 0) + 1\n",
    "            pred_counts[y_pred[i]] = pred_counts.get(y_pred[i], 0) + 1\n",
    "    for letter in pred_counts:\n",
    "        if letter in true_counts:\n",
    "            reward += 0.05 * min(pred_counts[letter], true_counts[letter])\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_guess_from_text(text):\n",
    "    \"\"\"\n",
    "    Extracts the guess from between <guess> and </guess> tags in the input text.\n",
    "    There should be only one pair of <guess>...</guess> tags.\n",
    "    Returns the guess as a string, stripped of whitespace.\n",
    "    Raises ValueError if the tags are missing or if there are multiple pairs.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    matches = re.findall(r'<guess>(.*?)</guess>', text, re.DOTALL)\n",
    "    if len(matches) != 1:\n",
    "        raise ValueError(f\"Expected exactly one <guess>...</guess> tag, found {len(matches)}.\")\n",
    "    return matches[0].strip()\n",
    "\n",
    "\n",
    "ttt = \"\"\"<think> some dumb thinking </think>\n",
    "<guess> HELLO </guess>\n",
    "\"\"\"\n",
    "extract_guess_from_text(ttt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.30000000000000004\n",
      "0.3\n",
      "0.2\n",
      "0.2\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(reward_wordle(\"CRANE\", \"CRANE\"))  # exact match, expect 1.0\n",
    "print(reward_wordle(\"CRANE\", \"CRONY\"))  # 3 correct positions (C, R, N), expect 0.3\n",
    "print(reward_wordle(\"CRANE\", \"CANER\"))  # 1 correct position (C), 4 correct letters in wrong positions, expect 0.1 + 0.05*4 = 0.3\n",
    "print(reward_wordle(\"CRANE\", \"PLANT\"))  # 1 correct position (A), 2 correct letters in wrong positions (N, E), expect 0.1 + 0.05*2 = 0.2\n",
    "print(reward_wordle(\"CRANE\", \"ABCDE\"))  # 1 correct position (E), 2 correct letters in wrong positions (C, A), expect 0.1 + 0.05*2 = 0.2\n",
    "print(reward_wordle(\"CRANE\", \"12345\"))  # invalid guess, expect 0.0\n",
    "print(reward_wordle(\"CRANE\", \"CRAN\"))   # invalid guess (length 4), expect 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "### 4. GRPO\n",
    "\n",
    "- implement GRPO loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.25.self_attn.q_proj.weight grad: 0.065693\n",
      "model.layers.25.self_attn.k_proj.weight grad: 0.056480\n",
      "model.layers.25.self_attn.v_proj.weight grad: 0.149816\n",
      "model.layers.25.self_attn.o_proj.weight grad: 0.114383\n",
      "model.layers.25.self_attn.q_norm.weight grad: 0.000832\n",
      "model.layers.25.self_attn.k_norm.weight grad: 0.001109\n",
      "model.layers.25.mlp.gate_proj.weight grad: 0.186269\n",
      "model.layers.25.mlp.up_proj.weight grad: 0.162070\n",
      "model.layers.25.mlp.down_proj.weight grad: 0.946478\n",
      "model.layers.25.input_layernorm.weight grad: 0.001677\n",
      "model.layers.25.post_attention_layernorm.weight grad: 0.000125\n",
      "model.layers.25.pre_feedforward_layernorm.weight grad: 0.013359\n",
      "model.layers.25.post_feedforward_layernorm.weight grad: 0.000112\n",
      "Model Gradient Norm ----> 1.000000\n"
     ]
    }
   ],
   "source": [
    "def print_grad_sum(model, verbose=False):\n",
    "    grads_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            # if grad is None, assume 0\n",
    "            if param.grad is None:\n",
    "                param.grad = torch.zeros_like(param)\n",
    "            grads_norms.append(param.grad.norm().item())\n",
    "            if verbose:\n",
    "                print(f\"{name} grad: {param.grad.norm().item():.6f}\")\n",
    "    if len(grads_norms) > 0:\n",
    "        grads_norms = np.array(grads_norms) ** 2\n",
    "        model_grad_norm = np.sqrt(grads_norms.sum())\n",
    "        print(f\"Model Gradient Norm ----> {model_grad_norm:.6f}\")\n",
    "    else:\n",
    "        print(\"No trainable parameters found\")\n",
    "\n",
    "print_grad_sum(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight, trainable: False, grad: 0.000000, no. of params: 301989888\n",
      "model.layers.0.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.0.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.0.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.0.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.0.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.0.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.0.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.0.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.0.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.0.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.0.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.0.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.0.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.1.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.1.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.1.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.1.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.1.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.1.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.1.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.1.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.1.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.1.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.1.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.1.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.1.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.2.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.2.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.2.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.2.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.2.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.2.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.2.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.2.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.2.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.2.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.2.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.2.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.2.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.3.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.3.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.3.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.3.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.3.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.3.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.3.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.3.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.3.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.3.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.3.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.3.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.3.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.4.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.4.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.4.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.4.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.4.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.4.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.4.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.4.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.4.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.4.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.4.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.4.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.4.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.5.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.5.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.5.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.5.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.5.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.5.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.5.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.5.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.5.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.5.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.5.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.5.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.5.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.6.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.6.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.6.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.6.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.6.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.6.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.6.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.6.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.6.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.6.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.6.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.6.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.6.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.7.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.7.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.7.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.7.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.7.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.7.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.7.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.7.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.7.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.7.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.7.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.7.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.7.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.8.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.8.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.8.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.8.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.8.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.8.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.8.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.8.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.8.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.8.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.8.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.8.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.8.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.9.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.9.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.9.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.9.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.9.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.9.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.9.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.9.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.9.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.9.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.9.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.9.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.9.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.10.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.10.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.10.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.10.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.10.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.10.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.10.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.10.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.10.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.10.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.10.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.10.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.10.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.11.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.11.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.11.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.11.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.11.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.11.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.11.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.11.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.11.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.11.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.11.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.11.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.11.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.12.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.12.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.12.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.12.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.12.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.12.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.12.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.12.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.12.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.12.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.12.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.12.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.12.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.13.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.13.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.13.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.13.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.13.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.13.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.13.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.13.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.13.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.13.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.13.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.13.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.13.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.14.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.14.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.14.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.14.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.14.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.14.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.14.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.14.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.14.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.14.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.14.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.14.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.14.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.15.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.15.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.15.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.15.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.15.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.15.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.15.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.15.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.15.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.15.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.15.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.15.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.15.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.16.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.16.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.16.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.16.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.16.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.16.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.16.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.16.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.16.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.16.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.16.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.16.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.16.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.17.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.17.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.17.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.17.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.17.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.17.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.17.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.17.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.17.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.17.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.17.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.17.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.17.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.18.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.18.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.18.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.18.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.18.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.18.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.18.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.18.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.18.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.18.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.18.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.18.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.18.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.19.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.19.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.19.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.19.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.19.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.19.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.19.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.19.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.19.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.19.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.19.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.19.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.19.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.20.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.20.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.20.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.20.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.20.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.20.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.20.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.20.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.20.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.20.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.20.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.20.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.20.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.21.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.21.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.21.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.21.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.21.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.21.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.21.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.21.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.21.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.21.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.21.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.21.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.21.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.22.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.22.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.22.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.22.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.22.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.22.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.22.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.22.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.22.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.22.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.22.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.22.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.22.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.23.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.23.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.23.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.23.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.23.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.23.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.23.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.23.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.23.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.23.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.23.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.23.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.23.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.24.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.24.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.24.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.24.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.24.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.24.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.24.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.24.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.24.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.24.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.24.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.24.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.24.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.25.self_attn.q_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.25.self_attn.k_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.25.self_attn.v_proj.weight, trainable: False, grad: 0.000000, no. of params: 294912\n",
      "model.layers.25.self_attn.o_proj.weight, trainable: False, grad: 0.000000, no. of params: 1179648\n",
      "model.layers.25.self_attn.q_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.25.self_attn.k_norm.weight, trainable: False, grad: 0.000000, no. of params: 256\n",
      "model.layers.25.mlp.gate_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.25.mlp.up_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.25.mlp.down_proj.weight, trainable: False, grad: 0.000000, no. of params: 7962624\n",
      "model.layers.25.input_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.25.post_attention_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.25.pre_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.layers.25.post_feedforward_layernorm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n",
      "model.norm.weight, trainable: False, grad: 0.000000, no. of params: 1152\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}, trainable: {param.requires_grad}, grad: {param.grad.norm().item():.6f}, no. of params: {param.numel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.25.self_attn.q_proj.weight grad: 0.000000\n",
      "model.layers.25.self_attn.k_proj.weight grad: 0.000000\n",
      "model.layers.25.self_attn.v_proj.weight grad: 0.000000\n",
      "model.layers.25.self_attn.o_proj.weight grad: 0.000000\n",
      "model.layers.25.self_attn.q_norm.weight grad: 0.000000\n",
      "model.layers.25.self_attn.k_norm.weight grad: 0.000000\n",
      "model.layers.25.mlp.gate_proj.weight grad: 0.000000\n",
      "model.layers.25.mlp.up_proj.weight grad: 0.000000\n",
      "model.layers.25.mlp.down_proj.weight grad: 0.000000\n",
      "model.layers.25.input_layernorm.weight grad: 0.000000\n",
      "model.layers.25.post_attention_layernorm.weight grad: 0.000000\n",
      "model.layers.25.pre_feedforward_layernorm.weight grad: 0.000000\n",
      "model.layers.25.post_feedforward_layernorm.weight grad: 0.000000\n",
      "Grad norm mean ----> 0.000000\n"
     ]
    }
   ],
   "source": [
    "# set only last layer of model to trainable\n",
    "# set model params to non-trainable\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"model.layers.25\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "print_grad_sum(model, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_log_prob(model, seq_ids, output_masks):\n",
    "    \"\"\"\n",
    "    Calculate model sequence log probability of OUTPUT given INPUT: P(output | input)\n",
    "    \n",
    "    Args:\n",
    "        model: Model to use for inference\n",
    "        seq_ids: Tensor of shape (B, seq_len) containing token ids of generated sequence belonging to same input\n",
    "        output_masks: Tensor of shape (B, seq_len) containing mask for ouptput positions. (input and padding positions in seq_ids are set to 0)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (B,) containing log probabilities for each outputsequence in batch\n",
    "        \n",
    "    Note:\n",
    "        Uses model.forward() to get logits for sequence, then uses log_softmax to get log probabilities.\n",
    "        Using model.forward() helps track gradients of output tensor for backprop.\n",
    "    \"\"\"\n",
    "    # opmask: 0 0 0 0 0 1 1 1 1 1 1     (masks of output positions, ignore padding tokens)\n",
    "    # input : i n p u t o u t p u t     (model input seq)\n",
    "    # output: n p u t o u t p u t -     (get logit for output tokens, ie. model probablity)\n",
    "    # masks:  0 0 0 0 1 1 1 1 1 1 0     (left shift by 1 to get logits for output tokens)\n",
    "    \n",
    "    attention_mask = torch.ones(1, seq_ids.shape[-1]).to(device)\n",
    "    # forward pass\n",
    "    output = model(\n",
    "        input_ids=seq_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "    logits = output.logits\n",
    "    token_log_prob = torch.log_softmax(logits, dim=-1)              # (B, seq_len, V)  -> log_prob for each token in seq\n",
    "    output_indices = torch.roll(seq_ids, shifts=-1, dims=-1)        # (B, seq_len) -> left shift by 1 to get indices for output tokens\n",
    "    token_log_prob = token_log_prob.gather(\n",
    "        index=output_indices.unsqueeze(-1),                         # (B, seq_len, 1)  -> (B, seq_len, 1) \n",
    "        dim=-1\n",
    "    )                                                               # (B, seq_len, 1)  -> log_prob for seq tokens\n",
    "    token_log_prob = token_log_prob.squeeze()                                   # (B, seq_len)\n",
    "    masks = torch.roll(output_masks, shifts=-1, dims=-1)            # left shift by 1\n",
    "    masks[:, -1] = 0                                                # pad last token with 0 as it is not output token\n",
    "    output_token_log_prob = token_log_prob * masks                  # (B, seq_len)  -> log_prob for output tokens     \n",
    "    output_log_prob = output_token_log_prob.sum(dim=-1)             # (B, 1)  -> sum of log_prob for output tokens\n",
    "    return output_log_prob\n",
    "\n",
    "\n",
    "\n",
    "def get_group_relative_reward_advantage(rewards: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate reward advantage of sequence with \"Group Relevative\" reward function.\n",
    "    \n",
    "    Args:\n",
    "        rewards: Tensor of shape (B,) containing reward values for generated sequences belonging to same input\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape (B,) containing reward advantage based on group relative reward function.\n",
    "    \"\"\"\n",
    "    mean_reward = rewards.mean()\n",
    "    std_reward = rewards.std()\n",
    "    advantages = (rewards - mean_reward) / std_reward\n",
    "    return advantages\n",
    "\n",
    "\n",
    "def get_kl_divergence(curr_log_prob, ref_log_prob):\n",
    "    \"\"\"\n",
    "    Calculate KL divergence between model and ref_model for sequence given input.\n",
    "\n",
    "    Args:\n",
    "        curr_log_prob: Tensor of shape (B,) containing log probabilities for current model\n",
    "        ref_log_prob: Tensor of shape (B,) containing log probabilities for reference model\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (B,) containing KL divergence for each outputsequence in batch\n",
    "    \"\"\"\n",
    "    kl_div = torch.exp(ref_log_prob - curr_log_prob) - (ref_log_prob - curr_log_prob) - 1\n",
    "    return kl_div\n",
    "    \n",
    "\n",
    "\n",
    "def grpo(curr_model, old_model, ref_model, seq_ids, output_masks, rewards, ep=0.2, beta=0.1):\n",
    "    \"\"\"\n",
    "    GRPO loss function\n",
    "\n",
    "    Args:\n",
    "        curr_model: current policy model - trained model weights\n",
    "        old_model: old policy model - old model weights from previous iteration\n",
    "        ref_model: Reference model base model - pretrained model weights\n",
    "        seq_ids: Tensor of shape (B, seq_len) containing token ids of generated sequence belonging to same input\n",
    "        output_masks: Tensor of shape (B, seq_len) containing mask for ouptput positions. (input and padding positions in seq_ids are set to 0)\n",
    "        rewards: Tensor of shape (B,) containing reward values for generated sequences belonging to same input\n",
    "        ep: clipping threshold for reward\n",
    "        beta: KL divergence regularization parameter\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (1,) containing GRPO loss for the batch\n",
    "    \"\"\"\n",
    "    curr_log_prob = get_model_log_prob(curr_model, seq_ids, output_masks)   # (B,) \n",
    "    with torch.no_grad():\n",
    "        # no grad for old and ref models\n",
    "        old_log_prob = get_model_log_prob(old_model, seq_ids, output_masks)\n",
    "        ref_log_prob = get_model_log_prob(ref_model, seq_ids, output_masks)\n",
    "    # policy objective\n",
    "    adv = get_group_relative_reward_advantage(rewards)                     \n",
    "    ratio = torch.exp(curr_log_prob - old_log_prob)                        \n",
    "    unclipped = ratio * adv                                                \n",
    "    clipped = torch.clamp(ratio, 1 - ep, 1 + ep) * adv                     \n",
    "    policy_obj = torch.min(unclipped, clipped)                             \n",
    "    # kl divergence\n",
    "    kl_div = get_kl_divergence(curr_log_prob, ref_log_prob)                \n",
    "    # grpo loss\n",
    "    grpo_loss = policy_obj - (beta * kl_div)                                # (B,)\n",
    "    grpo_loss = - grpo_loss.mean()                                          # (1,)\n",
    "    return grpo_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if get prod produces correct output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample shape: torch.Size([8, 38])\n",
      "Prob: tensor([8.6934e-07, 0.0000e+00, 2.0975e-05, 4.8292e-04, 8.7455e-05, 0.0000e+00,\n",
      "        4.8703e-12, 0.0000e+00], device='mps:0', grad_fn=<ExpBackward0>)\n",
      "Log prob: tensor([ -13.9555, -344.4367,  -10.7722,   -7.6357,   -9.3444, -218.2783,\n",
      "         -26.0479, -455.4019], device='mps:0', grad_fn=<SumBackward1>)\n",
      "log_prob requires grad: True\n"
     ]
    }
   ],
   "source": [
    "# test get_model_prob\n",
    "input_len = input_pt.input_ids.shape[1]\n",
    "print(f\"sample shape: {sample_gen.sequences.shape}\")\n",
    "output_masks = torch.ones(sample_gen.sequences.shape).to(device)\n",
    "output_masks[:, :input_len] = 0 # mask input tokens\n",
    "# TODO: mask padding tokens\n",
    "\n",
    "log_prob = get_model_log_prob(model, sample_gen.sequences, output_masks)\n",
    "print(f\"Prob: {torch.exp(log_prob)}\")\n",
    "print(f\"Log prob: {log_prob}\")\n",
    "print(f\"log_prob requires grad: {log_prob.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test gradients of masked position is zero; we only want to learning from output token, not the input and padding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output probability: tensor([0.3907], device='mps:0', grad_fn=<ExpBackward0>)\n",
      "log prob: tensor([-1.2838e+01, -1.1584e+00, -2.1250e+00, -5.1828e+00, -1.2865e+00,\n",
      "        -3.1996e-02, -1.5039e-03, -6.3487e-02, -1.4876e-04, -8.3659e-01,\n",
      "        -3.5763e-07, -5.0759e-03, -6.6636e-05, -5.2772e-04, -3.0656e-04,\n",
      "        -3.9765e+01], device='mps:0', grad_fn=<SqueezeBackward0>)\n",
      "loss: tensor([-0.0117], device='mps:0', grad_fn=<NegBackward0>)\n",
      "Logit grads before backward pass: None\n",
      "Log_prob grads before backward pass: None\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Backward pass done\n",
      "Loss grad: tensor([1.], device='mps:0')\n",
      "Prob grad: tensor([-0.0300], device='mps:0')\n",
      "log_prob grad: tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0117, -0.0117, -0.0117,\n",
      "        -0.0117, -0.0117, -0.0117, -0.0117, -0.0117, -0.0117, -0.0117, -0.0000],\n",
      "       device='mps:0')\n",
      "Logits grad shape: torch.Size([1, 16, 262144])\n",
      "Logits grad: \n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [4.2454e-30, 9.8360e-19, 7.1250e-24,  ..., 2.3541e-30,\n",
      "          1.7751e-30, 2.6256e-30],\n",
      "         [3.4140e-23, 2.0393e-15, 3.3337e-17,  ..., 6.1622e-24,\n",
      "          5.8626e-24, 6.7516e-24],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]], device='mps:0')\n",
      "Input token grads: tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='mps:0')\n",
      "Input token grads shape: torch.Size([1, 5, 262144])\n",
      "Max absolute gradient for input tokens: 0.0000000000\n",
      "Sum of input token gradients: 0.0000000000\n",
      "Are all input token gradients zero? True\n",
      "Output token grads shape: torch.Size([1, 11, 262144])\n",
      "Output token grads: tensor([[[6.4773e-24, 1.6008e-14, 1.8594e-16,  ..., 4.8978e-24,\n",
      "          4.6291e-24, 5.2035e-24],\n",
      "         [5.5983e-23, 5.1434e-17, 6.2351e-16,  ..., 3.3200e-23,\n",
      "          2.9717e-23, 4.0099e-23],\n",
      "         [2.0518e-28, 1.6199e-19, 6.6979e-22,  ..., 5.9036e-29,\n",
      "          5.6841e-29, 6.9751e-29],\n",
      "         ...,\n",
      "         [4.2454e-30, 9.8360e-19, 7.1250e-24,  ..., 2.3541e-30,\n",
      "          1.7751e-30, 2.6256e-30],\n",
      "         [3.4140e-23, 2.0393e-15, 3.3337e-17,  ..., 6.1622e-24,\n",
      "          5.8626e-24, 6.7516e-24],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]], device='mps:0')\n",
      "Max absolute gradient for output tokens: 0.0066443873\n",
      "Number of non-zero output gradients: 3675\n"
     ]
    }
   ],
   "source": [
    "def test_masked_gradients(model):\n",
    "    \"\"\"Test if gradients for masked positions are zero\"\"\"\n",
    "    \n",
    "    # Clear existing gradients\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass with sequences that require gradients\n",
    "    sample_input = \"Tell me a joke.\"\n",
    "    input_pt = tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "    input_len = input_pt.input_ids.shape[1]\n",
    "    sample_gen = model.generate(\n",
    "        **input_pt,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.5,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "    )\n",
    "    output_masks = torch.ones(sample_gen.sequences.shape).to(device)\n",
    "    output_masks[:, :input_len] = 0\n",
    "    output_masks.requires_grad = True\n",
    "    output_masks.retain_grad()\n",
    "    sequences_with_grad = sample_gen.sequences\n",
    "    p, log_prob, logits = get_model_log_prob(model, sequences_with_grad, output_masks)\n",
    "    log_prob.retain_grad()\n",
    "    logits.retain_grad()\n",
    "    p.retain_grad()\n",
    "    print(f\"Output probability: {p}\")\n",
    "    print(f\"log prob: {log_prob}\")\n",
    "\n",
    "    # some simple loss function like grpo\n",
    "    loss = - (p / (input_len + 1) * 0.21)\n",
    "    loss.retain_grad()\n",
    "    print(f\"loss: {loss}\")\n",
    "\n",
    "    # Grads before backward pass\n",
    "    print(f\"Logit grads before backward pass: {logits.grad}\")\n",
    "    print(f\"Log_prob grads before backward pass: {log_prob.grad}\")\n",
    "    \n",
    "    # Backward pass\n",
    "    print(\"-\"*100)\n",
    "    loss.backward()\n",
    "    print(\"Backward pass done\")\n",
    "    \n",
    "    print(f\"Loss grad: {loss.grad}\")\n",
    "    print(f\"Prob grad: {p.grad}\")\n",
    "    print(f\"log_prob grad: {log_prob.grad}\")\n",
    "    # Check if gradients for input positions are zero\n",
    "    logits_grad = logits.grad\n",
    "    print(f\"Logits grad shape: {logits_grad.shape}\")\n",
    "    print(f\"Logits grad: \\n{logits_grad}\")\n",
    "\n",
    "    # Check input token gradients (should be zero)\n",
    "    input_token_grads = logits_grad[:, :input_len-1]\n",
    "    print(f\"Input token grads: {input_token_grads}\")\n",
    "    print(f\"Input token grads shape: {input_token_grads.shape}\")\n",
    "    print(f\"Max absolute gradient for input tokens: {input_token_grads.abs().max().item():.10f}\")\n",
    "    print(f\"Sum of input token gradients: {input_token_grads.sum().item():.10f}\")\n",
    "    print(f\"Are all input token gradients zero? {torch.allclose(input_token_grads, torch.zeros_like(input_token_grads), atol=1e-10)}\")\n",
    "    \n",
    "    # Check output token gradients (should be non-zero)\n",
    "    output_token_grads = logits_grad[:, input_len-1:]\n",
    "    print(f\"Output token grads shape: {output_token_grads.shape}\")\n",
    "    print(f\"Output token grads: {output_token_grads}\")\n",
    "    print(f\"Max absolute gradient for output tokens: {output_token_grads.abs().max().item():.10f}\")\n",
    "    print(f\"Number of non-zero output gradients: {(output_token_grads.abs() > 1e-10).sum().item()}\")\n",
    "    \n",
    "    return logits_grad\n",
    "\n",
    "# Test with your data\n",
    "_ = test_masked_gradients(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample shape: torch.Size([1, 16])\n",
      "Log prob: tensor([-1.5540], device='mps:0', grad_fn=<SumBackward1>)\n",
      "Prob: tensor([0.2114], device='mps:0', grad_fn=<ExpBackward0>)\n",
      "KLD: tensor([0.], device='mps:0', grad_fn=<SubBackward0>)\n",
      " KL divergence is zero as expected for same model comparison\n"
     ]
    }
   ],
   "source": [
    "# test get_kl_divergence for same model\n",
    "# test get_model_prob\n",
    "sample_input = \"Tell me a joke.\"\n",
    "input_pt = tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "input_len = input_pt.input_ids.shape[1]\n",
    "sample_gen = model.generate(\n",
    "        **input_pt,\n",
    "        max_new_tokens=10,\n",
    "        temperature=0.5,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    ")\n",
    "\n",
    "print(f\"sample shape: {sample_gen.sequences.shape}\")\n",
    "input_len = input_pt.input_ids.shape[1]\n",
    "output_masks = torch.ones(sample_gen.sequences.shape).to(device)\n",
    "output_masks[:, :input_len] = 0 # mask input tokens\n",
    "\n",
    "log_prob = get_model_log_prob(model, sample_gen.sequences, output_masks)\n",
    "print(f\"Log prob: {log_prob}\")\n",
    "print(f\"Prob: {torch.exp(log_prob)}\")\n",
    "\n",
    "kl_div = get_kl_divergence(log_prob, log_prob)\n",
    "print(f\"KLD: {kl_div}\")\n",
    "# Assert that KL divergence is zero when comparing same model to itself\n",
    "assert kl_div == 0.0, f\"KL divergence should be zero for same model, got {kl_div}\"\n",
    "print(\" KL divergence is zero as expected for same model comparison\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sample sequences and prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_len: 6\n",
      "Shape of sample_gen.sequences: torch.Size([8, 38])\n",
      "sample_gen.sequences:\n",
      "tensor([[     2,  54593,    786,    496,  31481, 236761,    108,  11355,   1602,\n",
      "            506,  12480,   4071,    506,  37421, 236881,    108,   2021,    974,\n",
      "            531,    506,   1032,  13307, 236888,    108,   7243,    108,  19058,\n",
      "         236764,    600, 236789, 236751,    496,   1535,    886, 236888, 236743,\n",
      "            108,   3689],\n",
      "        [     2,  54593,    786,    496,  31481, 236761,    108,  11355,   1602,\n",
      "            506,  55134,  47129,   3345,    614,   8054, 236881,    108,   1390,\n",
      "           8468,    668,    691,  15647,    528,    914,   2135, 236888,    107,\n",
      "            106,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0],\n",
      "        [     2,  54593,    786,    496,  31481, 236761,    108,  11355,   1602,\n",
      "            506,  30979,   3798,   1024, 236881,    108,  17574,    625,    691,\n",
      "           1156,  20718, 236888,    108,   7243,    108,  19058, 236764,    564,\n",
      "         236789,    859,   2056,   1570, 236888,   5715, 236789, 236751,    496,\n",
      "          31481, 236787],\n",
      "        [     2,  54593,    786,    496,  31481, 236761,    108,  11355,   1602,\n",
      "            506,  55134,  47129,   3345,    614,   8054, 236881,    108,  17574,\n",
      "            668,    691,  15647,    528,    914,   2135, 236888,    108,   7243,\n",
      "            108,  15562,    611,   4059,    600,  31481, 236881,    138,   6294,\n",
      "            611,   1461],\n",
      "        [     2,  54593,    786,    496,  31481, 236761,    108,  11355,   1602,\n",
      "            506,  12480,   4071,    506,  37421, 236881,    108,   1390,   2282,\n",
      "            974,    531,    506,   1032,  13307, 236888,    108,   7243,    108,\n",
      "          38786,    611,   1133,    786,    531,   3442,    611,   2264,  31481,\n",
      "         236881,    106],\n",
      "        [     2,  54593,    786,    496,  31481, 236761,    108,  11355,   1602,\n",
      "            506,  30979,   3798,   1024, 236881,    108,  17574,    625,    691,\n",
      "           1156,  20718, 236761,    108,   7243,    108,  38786,    611,   1133,\n",
      "           2264,  31481, 236881,    107,    106,      0,      0,      0,      0,\n",
      "              0,      0],\n",
      "        [     2,  54593,    786,    496,  31481, 236761,    108,  11355,   1602,\n",
      "            506,  30979,   3798,   1024, 236881,    108,  17574,    625,    691,\n",
      "           1156,  20718, 236888,    108,   7243,    108, 236769,  11896,    496,\n",
      "           1944,    886, 236761,    138, 236777, 236789, 236757,   2036,   2844,\n",
      "            580,   1091],\n",
      "        [     2,  54593,    786,    496,  31481, 236761,    108,  11355,   1602,\n",
      "            506,  30979,   3798,   1024, 236881,    108,  17574,    625,    691,\n",
      "           1156,  20718, 236888,    107,    106,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0]], device='mps:0')\n",
      "pad_token_id: 0\n",
      "output_masks:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]], device='mps:0')\n",
      "0 Tell me a joke.\n",
      "\n",
      "Why did the chicken cross the playground?\n",
      "\n",
      "To get to the other slide!\n",
      "\n",
      "---\n",
      "\n",
      "Okay, that's a good one! \n",
      "\n",
      "What\n",
      "1 Tell me a joke.\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "\n",
      "... Because he was outstanding in his field!\n",
      "\n",
      "2 Tell me a joke.\n",
      "\n",
      "Why did the bicycle fall over?\n",
      "\n",
      "Because it was two tired!\n",
      "\n",
      "---\n",
      "\n",
      "Okay, I'll try again! Here's a joke:\n",
      "3 Tell me a joke.\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n",
      "\n",
      "---\n",
      "\n",
      "Did you enjoy that joke?  Do you want\n",
      "4 Tell me a joke.\n",
      "\n",
      "Why did the chicken cross the playground?\n",
      "\n",
      "... To get to the other slide!\n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to tell you another joke?\n",
      "5 Tell me a joke.\n",
      "\n",
      "Why did the bicycle fall over?\n",
      "\n",
      "Because it was two tired.\n",
      "\n",
      "---\n",
      "\n",
      "Would you like another joke?\n",
      "\n",
      "6 Tell me a joke.\n",
      "\n",
      "Why did the bicycle fall over?\n",
      "\n",
      "Because it was two tired!\n",
      "\n",
      "---\n",
      "\n",
      "(Just a small one.  I'm still working on them\n",
      "7 Tell me a joke.\n",
      "\n",
      "Why did the bicycle fall over?\n",
      "\n",
      "Because it was two tired!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate sample sequences for single input\n",
    "num_return_sequences = 8\n",
    "sample_input = \"Tell me a joke.\"\n",
    "input_pt = tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "input_len = input_pt.input_ids.shape[1]\n",
    "print(f\"input_len: {input_len}\")\n",
    "sample_gen = model.generate(\n",
    "        **input_pt,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        max_new_tokens=32,\n",
    "        temperature=1,\n",
    "        return_dict_in_generate=True,\n",
    ")\n",
    "print(f\"Shape of sample_gen.sequences: {sample_gen.sequences.shape}\")\n",
    "input_len = input_pt.input_ids.shape[1]\n",
    "print(f\"sample_gen.sequences:\")\n",
    "print(sample_gen.sequences)\n",
    "\n",
    "# Set input and padding tokens to zero in output_masks\n",
    "output_masks = torch.ones(sample_gen.sequences.shape).to(device)\n",
    "output_masks[:, :input_len] = 0 # mask input tokens\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"pad_token_id: {pad_token_id}\")\n",
    "if pad_token_id is not None:\n",
    "    padding_mask = (sample_gen.sequences == pad_token_id)\n",
    "    output_masks[padding_mask] = 0\n",
    "print(f\"output_masks:\")\n",
    "print(output_masks)\n",
    "\n",
    "# decode the sequences\n",
    "for i in range(sample_gen.sequences.shape[0]):\n",
    "    print(i, tokenizer.decode(sample_gen.sequences[i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the first step of training, curr, old, ref models are the same.\n",
    "```\n",
    "ratio_policy = 1.\n",
    "Thus, loss = mean(adv) = 0\n",
    "```\n",
    "But, the gradient will still be NON-zero, ensuring that the model will learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Gradient Norm ----> 0.000000\n",
      " When current, old and ref models are same  GRPO loss should be equal to negative mean of advantages\n",
      "rewards: tensor([-2.0928, -0.9464, -1.2513, -0.4060, -2.0444,  0.5433,  0.7662,  1.6418],\n",
      "       device='mps:0')\n",
      "adv: tensor([-1.1895, -0.3473, -0.5713,  0.0497, -1.1540,  0.7472,  0.9109,  1.5543],\n",
      "       device='mps:0')\n",
      "advantages mean: -0.000000\n",
      "GRPO loss: -0.000000\n",
      " GRPO loss is equal to negative mean of advantages\n",
      " GRPO loss is close to zero, during first step of training\n",
      "model.layers.25.self_attn.q_proj.weight grad: 112.187042\n",
      "model.layers.25.self_attn.k_proj.weight grad: 113.542786\n",
      "model.layers.25.self_attn.v_proj.weight grad: 298.359863\n",
      "model.layers.25.self_attn.o_proj.weight grad: 207.357681\n",
      "model.layers.25.self_attn.q_norm.weight grad: 1.726742\n",
      "model.layers.25.self_attn.k_norm.weight grad: 2.308616\n",
      "model.layers.25.mlp.gate_proj.weight grad: 359.957367\n",
      "model.layers.25.mlp.up_proj.weight grad: 310.338593\n",
      "model.layers.25.mlp.down_proj.weight grad: 1605.714478\n",
      "model.layers.25.input_layernorm.weight grad: 4.566746\n",
      "model.layers.25.post_attention_layernorm.weight grad: 0.268382\n",
      "model.layers.25.pre_feedforward_layernorm.weight grad: 30.723366\n",
      "model.layers.25.post_feedforward_layernorm.weight grad: 0.194476\n",
      "Model Gradient Norm ----> 1721.239450\n",
      " However, gradients are non-zero, even though loss is zero\n",
      "After clipping gradients:\n",
      "model.layers.25.self_attn.q_proj.weight grad: 0.065178\n",
      "model.layers.25.self_attn.k_proj.weight grad: 0.065966\n",
      "model.layers.25.self_attn.v_proj.weight grad: 0.173340\n",
      "model.layers.25.self_attn.o_proj.weight grad: 0.120470\n",
      "model.layers.25.self_attn.q_norm.weight grad: 0.001003\n",
      "model.layers.25.self_attn.k_norm.weight grad: 0.001341\n",
      "model.layers.25.mlp.gate_proj.weight grad: 0.209127\n",
      "model.layers.25.mlp.up_proj.weight grad: 0.180299\n",
      "model.layers.25.mlp.down_proj.weight grad: 0.932883\n",
      "model.layers.25.input_layernorm.weight grad: 0.002653\n",
      "model.layers.25.post_attention_layernorm.weight grad: 0.000156\n",
      "model.layers.25.pre_feedforward_layernorm.weight grad: 0.017850\n",
      "model.layers.25.post_feedforward_layernorm.weight grad: 0.000113\n",
      "Model Gradient Norm ----> 1.000000\n",
      " Gradients are clipped to 1.0\n"
     ]
    }
   ],
   "source": [
    "# test grpo loss function\n",
    "model.zero_grad()\n",
    "print_grad_sum(model)\n",
    "rewards = torch.randn(num_return_sequences).to(device)\n",
    "print(\" When current, old and ref models are same  GRPO loss should be equal to negative mean of advantages\")\n",
    "adv = get_group_relative_reward_advantage(rewards)\n",
    "print(f\"rewards: {rewards}\")\n",
    "print(f\"adv: {adv}\")\n",
    "print(f\"advantages mean: {-adv.mean():.6f}\")\n",
    "\n",
    "grpo_loss = grpo(model, model, model, sample_gen.sequences, output_masks, rewards)\n",
    "print(f\"GRPO loss: {grpo_loss:.6f}\")\n",
    "\n",
    "# assert\n",
    "assert grpo_loss == -adv.mean(), f\"GRPO loss should be equal to mean of advantages, got {grpo_loss} and {adv.mean()}\"\n",
    "assert abs(grpo_loss) < 1e-6, f\"GRPO loss should be approximately zero, got {grpo_loss}\"\n",
    "\n",
    "print(\" GRPO loss is equal to negative mean of advantages\")\n",
    "print(\" GRPO loss is close to zero, during first step of training\")\n",
    "\n",
    "# check if grads are NON-ZERO\n",
    "grpo_loss.backward()\n",
    "print_grad_sum(model, verbose=True)\n",
    "print(\" However, gradients are non-zero, even though loss is zero\")\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "print(f\"After clipping gradients:\")\n",
    "print_grad_sum(model, verbose=True)\n",
    "print(\" Gradients are clipped to 1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "### 5. Training Loop\n",
    "\n",
    "GRPO training loop\n",
    "```\n",
    "for sample in dataset\n",
    "    1.  generations -> model.generate(sample, n=8)\n",
    "        batch generations togethers with padding\n",
    "        create output token mask, \n",
    "    2.  reward, advantage --> score (generations)\n",
    "        loss -> grpo; seq_prob; kl\n",
    "    3.  backprop -> loss.backward() \n",
    "        optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "adam = optim.Adam(model.parameters())\n",
    "adam.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output sequence (len:16)--> Tell me a joke.\n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "input_len: 6\n",
      "Generated sequence (len:10)--> \n",
      "\n",
      "Why did the scarecrow win an award?\n",
      "sum_gen_seq_log_prob: -0.9397\n",
      "gen_seq_prob: 39.07%\n",
      "\"\n",
      "\n",
      "\"\t| token 108 prob: 96.85% | log prob: -0.032\n",
      "\"Why\"\t| token 11355 prob: 99.85% | log prob: -0.001504\n",
      "\" did\"\t| token 1602 prob: 93.85% | log prob: -0.06349\n",
      "\" the\"\t| token 506 prob: 99.99% | log prob: -0.0001488\n",
      "\" scare\"\t| token 55134 prob: 43.32% | log prob: -0.8366\n",
      "\"crow\"\t| token 47129 prob: 100.00% | log prob: -3.576e-07\n",
      "\" win\"\t| token 3345 prob: 99.49% | log prob: -0.005076\n",
      "\" an\"\t| token 614 prob: 99.99% | log prob: -6.664e-05\n",
      "\" award\"\t| token 8054 prob: 99.95% | log prob: -0.0005277\n",
      "\"?\"\t| token 236881 prob: 99.97% | log prob: -0.0003066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3907, device='mps:0', grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "adam.zero_grad()\n",
    "\n",
    "sample_input = \"Tell me a joke.\"\n",
    "input_pt = tokenizer(sample_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# sample generation (auto-regressive)\n",
    "sample_gen = model.generate(\n",
    "    **input_pt,\n",
    "    max_new_tokens=10,\n",
    "    temperature=0.5,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "print(f\"Output sequence (len:{sample_gen.sequences[0].shape[0]})--> {tokenizer.decode(sample_gen.sequences[0], skip_special_tokens=True)}\")\n",
    "input_len = input_pt.input_ids.shape[1]\n",
    "print(f\"input_len: {input_len}\")\n",
    "seq_len = sample_gen.sequences[0].shape[0]\n",
    "gen_seq = sample_gen.sequences[0][input_len:]\n",
    "print(f\"Generated sequence (len:{gen_seq.shape[0]})--> {tokenizer.decode(gen_seq, skip_special_tokens=True)}\")\n",
    "\n",
    "# model forward pass - to get logits for the generated sequence\n",
    "attention_mask = torch.ones(1, seq_len)\n",
    "output_logits = model(\n",
    "    input_ids=sample_gen.sequences,\n",
    "    attention_mask=attention_mask,\n",
    ")\n",
    "# sum of log_probs for ids in sample_gen.sequences[0]\n",
    "gen_seq_logits = output_logits.logits[:,input_len-1:-1,:]   # ***IMP*** logits for generated sequence input sequence left shifted by 1\n",
    "gen_seq_scores = torch.log_softmax(gen_seq_logits, dim=-1)\n",
    "gen_seq_ids = sample_gen.sequences.unsqueeze(-1)[:,input_len:,:]    # ids of generated sequence\n",
    "gen_seq_log_prob = gen_seq_scores.gather(dim=2, index=gen_seq_ids).squeeze()\n",
    "\n",
    "sum_gen_seq_log_prob = gen_seq_log_prob.sum()\n",
    "gen_seq_prob = torch.exp(sum_gen_seq_log_prob)\n",
    "print(f\"sum_gen_seq_log_prob: {sum_gen_seq_log_prob:.4}\")\n",
    "print(f\"gen_seq_prob: {gen_seq_prob:.2%}\")\n",
    "# print prob of each token in gen_seq\n",
    "for i in range(gen_seq.shape[0]):\n",
    "    print(f\"\\\"{tokenizer.decode(gen_seq[i])}\\\"\\t| token {gen_seq[i]} prob: {torch.exp(gen_seq_log_prob[i]):.2%} | log prob: {gen_seq_log_prob[i]:.4}\")\n",
    "gen_seq_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\n",
    "\n",
    "\"\t| token 108 prob: 100.00%\n",
    "\"Why\"\t| token 11355 prob: 100.00%\n",
    "\" did\"\t| token 1602 prob: 100.00%\n",
    "\" the\"\t| token 506 prob: 100.00%\n",
    "\" bicycle\"\t| token 30979 prob: 35.67%\n",
    "\" fall\"\t| token 3798 prob: 100.00%\n",
    "\" over\"\t| token 1024 prob: 100.00%\n",
    "\"?\"\t| token 236881 prob: 100.00%\n",
    "\"\n",
    "\n",
    "\"\t| token 108 prob: 94.77%\n",
    "\"Because\"\t| token 17574 prob: 100.00%\n",
    "Sequence 1 log prob: -1.085\n",
    "Sequence 1 prob: 33.80%\n",
    "Sequence 1 string: \n",
    "\n",
    "Why did the bicycle fall over?\n",
    "\n",
    "Because"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: -3.907\n",
      "model.embed_tokens.weight grad: 0.0\n",
      "model.layers.0.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.0.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.0.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.0.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.0.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.0.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.0.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.0.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.0.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.0.input_layernorm.weight grad: 0.0\n",
      "model.layers.0.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.0.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.0.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.1.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.1.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.1.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.1.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.1.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.1.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.1.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.1.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.1.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.1.input_layernorm.weight grad: 0.0\n",
      "model.layers.1.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.1.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.1.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.2.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.2.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.2.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.2.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.2.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.2.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.2.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.2.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.2.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.2.input_layernorm.weight grad: 0.0\n",
      "model.layers.2.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.2.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.2.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.3.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.3.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.3.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.3.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.3.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.3.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.3.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.3.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.3.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.3.input_layernorm.weight grad: 0.0\n",
      "model.layers.3.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.3.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.3.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.4.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.4.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.4.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.4.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.4.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.4.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.4.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.4.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.4.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.4.input_layernorm.weight grad: 0.0\n",
      "model.layers.4.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.4.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.4.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.5.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.5.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.5.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.5.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.5.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.5.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.5.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.5.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.5.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.5.input_layernorm.weight grad: 0.0\n",
      "model.layers.5.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.5.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.5.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.6.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.6.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.6.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.6.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.6.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.6.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.6.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.6.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.6.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.6.input_layernorm.weight grad: 0.0\n",
      "model.layers.6.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.6.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.6.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.7.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.7.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.7.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.7.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.7.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.7.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.7.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.7.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.7.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.7.input_layernorm.weight grad: 0.0\n",
      "model.layers.7.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.7.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.7.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.8.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.8.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.8.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.8.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.8.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.8.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.8.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.8.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.8.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.8.input_layernorm.weight grad: 0.0\n",
      "model.layers.8.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.8.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.8.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.9.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.9.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.9.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.9.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.9.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.9.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.9.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.9.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.9.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.9.input_layernorm.weight grad: 0.0\n",
      "model.layers.9.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.9.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.9.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.10.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.10.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.10.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.10.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.10.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.10.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.10.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.10.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.10.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.10.input_layernorm.weight grad: 0.0\n",
      "model.layers.10.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.10.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.10.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.11.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.11.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.11.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.11.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.11.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.11.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.11.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.11.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.11.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.11.input_layernorm.weight grad: 0.0\n",
      "model.layers.11.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.11.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.11.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.12.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.12.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.12.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.12.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.12.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.12.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.12.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.12.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.12.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.12.input_layernorm.weight grad: 0.0\n",
      "model.layers.12.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.12.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.12.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.13.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.13.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.13.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.13.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.13.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.13.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.13.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.13.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.13.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.13.input_layernorm.weight grad: 0.0\n",
      "model.layers.13.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.13.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.13.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.14.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.14.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.14.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.14.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.14.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.14.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.14.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.14.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.14.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.14.input_layernorm.weight grad: 0.0\n",
      "model.layers.14.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.14.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.14.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.15.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.15.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.15.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.15.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.15.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.15.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.15.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.15.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.15.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.15.input_layernorm.weight grad: 0.0\n",
      "model.layers.15.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.15.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.15.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.16.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.16.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.16.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.16.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.16.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.16.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.16.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.16.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.16.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.16.input_layernorm.weight grad: 0.0\n",
      "model.layers.16.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.16.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.16.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.17.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.17.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.17.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.17.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.17.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.17.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.17.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.17.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.17.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.17.input_layernorm.weight grad: 0.0\n",
      "model.layers.17.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.17.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.17.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.18.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.18.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.18.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.18.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.18.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.18.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.18.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.18.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.18.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.18.input_layernorm.weight grad: 0.0\n",
      "model.layers.18.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.18.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.18.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.19.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.19.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.19.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.19.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.19.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.19.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.19.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.19.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.19.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.19.input_layernorm.weight grad: 0.0\n",
      "model.layers.19.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.19.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.19.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.20.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.20.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.20.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.20.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.20.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.20.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.20.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.20.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.20.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.20.input_layernorm.weight grad: 0.0\n",
      "model.layers.20.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.20.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.20.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.21.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.21.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.21.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.21.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.21.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.21.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.21.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.21.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.21.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.21.input_layernorm.weight grad: 0.0\n",
      "model.layers.21.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.21.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.21.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.22.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.22.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.22.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.22.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.22.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.22.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.22.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.22.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.22.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.22.input_layernorm.weight grad: 0.0\n",
      "model.layers.22.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.22.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.22.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.23.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.23.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.23.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.23.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.23.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.23.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.23.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.23.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.23.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.23.input_layernorm.weight grad: 0.0\n",
      "model.layers.23.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.23.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.23.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.24.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.24.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.24.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.24.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.24.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.24.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.24.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.24.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.24.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.24.input_layernorm.weight grad: 0.0\n",
      "model.layers.24.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.24.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.24.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.25.self_attn.q_proj.weight grad: 0.0\n",
      "model.layers.25.self_attn.k_proj.weight grad: 0.0\n",
      "model.layers.25.self_attn.v_proj.weight grad: 0.0\n",
      "model.layers.25.self_attn.o_proj.weight grad: 0.0\n",
      "model.layers.25.self_attn.q_norm.weight grad: 0.0\n",
      "model.layers.25.self_attn.k_norm.weight grad: 0.0\n",
      "model.layers.25.mlp.gate_proj.weight grad: 0.0\n",
      "model.layers.25.mlp.up_proj.weight grad: 0.0\n",
      "model.layers.25.mlp.down_proj.weight grad: 0.0\n",
      "model.layers.25.input_layernorm.weight grad: 0.0\n",
      "model.layers.25.post_attention_layernorm.weight grad: 0.0\n",
      "model.layers.25.pre_feedforward_layernorm.weight grad: 0.0\n",
      "model.layers.25.post_feedforward_layernorm.weight grad: 0.0\n",
      "model.norm.weight grad: 0.0\n"
     ]
    }
   ],
   "source": [
    "# calculate loss\n",
    "loss = - (gen_seq_prob/0.1 * 1)\n",
    "print(f\"loss: {loss:.4}\")\n",
    "\n",
    "# calculate gradient\n",
    "# loss.backward()\n",
    "\n",
    "# print grad sum\n",
    "print_grad_sum(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_log_probs: tensor([-11.4805, -12.5969, -11.2116,  -9.4501,  -9.7526, -11.2985, -11.6984,\n",
      "        -11.4296], grad_fn=<SumBackward1>)\n",
      "copy_seq_log_probs: tensor([-11.4805, -12.5969, -11.2116,  -9.4501,  -9.7526, -11.2985, -11.6984,\n",
      "        -11.4296], grad_fn=<SumBackward1>)\n",
      "adv mean: 0.000000\n",
      "ratio: tensor([1., 1., 1., 1., 1., 1., 1., 1.], grad_fn=<ExpBackward0>)\n",
      "ratio mean: 1.000000\n",
      "loss: 0.0\n",
      "loss: 0.000000\n",
      "embed.weight: 0.345921, size: torch.Size([10, 16])\n",
      "decoder.weight: 2.347969, size: torch.Size([10, 16])\n",
      "decoder.bias: 0.629673, size: torch.Size([10])\n",
      "Gradient norms: [0.34592145681381226, 2.3479692935943604, 0.6296733021736145]\n",
      "\n",
      "Conclusion:\n",
      "During the first step of training, curr, old, ref models are the same. So the loss (aka. learning) will be zero.\n",
      "But, the gradient will still be non-zero, ensuring that the model will learn.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "\n",
    "# Dummy language model (simplified to linear for toy test)\n",
    "class ToyLM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        hidden = self.embed(input_ids)\n",
    "        logits = self.decoder(hidden)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        return log_probs\n",
    "\n",
    "# ---- SETUP ---- #\n",
    "vocab_size = 10\n",
    "seq_len = 5\n",
    "hidden_dim = 16\n",
    "G = 8  # Group size\n",
    "\n",
    "model = ToyLM(vocab_size, hidden_dim)\n",
    "model_copy = copy.deepcopy(model)\n",
    "\n",
    "# Create G sampled outputs (sequences)\n",
    "input_ids = torch.randint(0, vocab_size, (G, seq_len))  # [G, T]\n",
    "\n",
    "# Forward pass\n",
    "log_probs = model(input_ids)  # [G, T, V]\n",
    "token_log_probs = log_probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)  # log P(o_i|q)\n",
    "# Sequence log-probs\n",
    "seq_log_probs = token_log_probs.sum(dim=1)  # [G]\n",
    "\n",
    "copy_log_probs = model_copy(input_ids)\n",
    "copy_token_log_probs = copy_log_probs.gather(2, input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "copy_seq_log_probs = copy_token_log_probs.sum(dim=1)\n",
    "\n",
    "print(f\"seq_log_probs: {seq_log_probs}\")\n",
    "print(f\"copy_seq_log_probs: {copy_seq_log_probs}\")\n",
    "\n",
    "# Compute advantages (simulate standardized rewards)\n",
    "# rewards = torch.tensor([1.0, 0.5, -0.2, 0.7])\n",
    "rewards = torch.randn(G)\n",
    "adv = (rewards - rewards.mean()) / rewards.std()  # [G]\n",
    "print(f\"adv mean: {adv.mean():.6f}\")\n",
    "\n",
    "# GRPO loss component: - log_prob * advantage (REINFORCE-style)\n",
    "ratio = torch.exp(seq_log_probs - copy_seq_log_probs)\n",
    "print(f\"ratio: {ratio}\")\n",
    "print(f\"ratio mean: {ratio.mean():.6f}\")\n",
    "loss = - (ratio  * adv)\n",
    "loss = loss.mean()\n",
    "print(f\"loss: {loss}\")\n",
    "print(f\"loss: {loss:.6f}\")\n",
    "loss.backward()\n",
    "\n",
    "# ---- TEST ---- #\n",
    "grads = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"{name}: {param.grad.norm().item():.6f}, size: {param.grad.shape}\")\n",
    "        grads.append(param.grad.norm().item())\n",
    "\n",
    "print(\"Gradient norms:\", grads)\n",
    "\n",
    "print(f\"\"\"\n",
    "Conclusion:\n",
    "During the first step of training, curr, old, ref models are the same. So the loss (aka. learning) will be zero.\n",
    "But, the gradient will still be non-zero, ensuring that the model will learn.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
